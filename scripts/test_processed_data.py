#!/usr/bin/env python3
"""
Test des donnÃ©es preprocessÃ©es pour vÃ©rifier leur qualitÃ©.
"""

import torch
import json
from pathlib import Path
from transformers import AutoTokenizer

def test_processed_data():
    """Test des donnÃ©es preprocessÃ©es."""
    
    print("=== TEST DES DONNÃ‰ES PREPROCESSÃ‰ES ===\n")
    
    # Charger le manifeste
    manifest_path = Path("data/processed/training_manifest.json")
    with open(manifest_path, 'r', encoding='utf-8') as f:
        manifest = json.load(f)
    
    print(f"ğŸ“Š RÃ‰SUMÃ‰ DU DATASET")
    print(f"   Total Ã©chantillons: {manifest['total_samples']:,}")
    print(f"   Tokenizer: {manifest['tokenizer']}")
    print()
    
    # Charger le tokenizer
    tokenizer = AutoTokenizer.from_pretrained(manifest['tokenizer'])
    
    # Tester chaque dataset
    for modality, datasets in manifest['datasets'].items():
        print(f"ğŸ”¤ MODALITÃ‰: {modality.upper()}")
        
        for dataset_name, dataset_info in datasets.items():
            print(f"\nğŸ“¦ Dataset: {dataset_name}")
            print(f"   Ã‰chantillons: {dataset_info['total_samples']:,}")
            print(f"   IgnorÃ©s: {dataset_info['skipped_samples']:,}")
            print(f"   Longueur moyenne tokens: {dataset_info['avg_token_length']:.1f}")
            print(f"   Chunks: {len(dataset_info['chunk_files'])}")
            
            # Charger et tester le premier chunk
            first_chunk_path = dataset_info['chunk_files'][0]
            try:
                chunk_data = torch.load(first_chunk_path, map_location='cpu')
                print(f"   âœ… Chunk chargÃ©: {len(chunk_data)} Ã©chantillons")
                
                # Examiner quelques Ã©chantillons
                print(f"\n   ğŸ“‹ Ã‰CHANTILLONS ({dataset_name}):")
                for i, sample in enumerate(chunk_data[:3]):  # Premier 3
                    print(f"   [{i+1}] Tokens: {sample['token_length']}")
                    print(f"       Preview: {sample['text_preview'][:100]}...")
                    
                    # DÃ©coder les tokens pour vÃ©rifier
                    decoded = tokenizer.decode(sample['input_ids'][:50])  # Premier 50 tokens
                    print(f"       Decoded: {decoded[:100]}...")
                    print()
                    
            except Exception as e:
                print(f"   âŒ Erreur chargement chunk: {e}")
        print()
    
    # Test des splits
    splits_path = Path("data/processed/data_splits.json")
    if splits_path.exists():
        with open(splits_path, 'r', encoding='utf-8') as f:
            splits = json.load(f)
        
        print("ğŸ“‚ SPLITS DE DONNÃ‰ES")
        for modality, datasets in splits.items():
            for dataset_name, split_info in datasets.items():
                print(f"   {dataset_name}:")
                print(f"     Train chunks: {len(split_info['train_chunks'])}")
                print(f"     Val chunks: {len(split_info['val_chunks'])}")
                print(f"     Test chunks: {len(split_info['test_chunks'])}")
    
    print("\nâœ… TEST TERMINÃ‰ - DonnÃ©es prÃªtes pour l'entraÃ®nement!")

if __name__ == "__main__":
    test_processed_data()