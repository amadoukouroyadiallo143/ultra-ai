# Stage 1: Pré-entraînement Textuel
# Focus sur le backbone Mamba-2 avec données textuelles

# Model Configuration - Text Only Stage
model_name: "ultra-ai-stage1"
model_variant: "ultra-edge"  # Version test locale
d_model: 512
vocab_size: 50432
max_seq_length: 100000  # 100K tokens pour commencer

# Architecture - Text Focus
mamba_ratio: 0.90    # Plus de Mamba pour cette phase
attention_ratio: 0.10 # Attention minimale  
moe_ratio: 0.00      # MoE désactivé
multimodal_ratio: 0.00 # Pas de multimodal

# Mamba-2 Configuration (Optimisé text)
mamba_layers: 12
mamba_d_state: 128
mamba_d_conv: 4
mamba_expand: 2
mamba_dt_rank: "auto"

# Attention Configuration (Minimale)
attention_layers: 2
attention_heads: 8
attention_type: "linear"

# Training Configuration (Conservative)
batch_size: 2
micro_batch_size: 1
gradient_accumulation_steps: 32  # Effective batch = 64
max_gradient_accumulation_tokens: 500000

learning_rate: 0.0001
min_learning_rate: 0.00001
weight_decay: 0.01
max_grad_norm: 1.0
num_epochs: 2
warmup_ratio: 0.1

# Optimizer (Memory Efficient)
optimizer: "adamw"
beta1: 0.9
beta2: 0.95
eps: 0.00000001
fused_optimizer: true

# Scheduler
scheduler: "cosine"
warmup_steps: 2000

# Training Efficiency
mixed_precision: "fp16"  # Compatible GPU plus anciennes
gradient_checkpointing: true
use_flash_attention: false  # Désactivé pour compatibilité
compile_model: false  # Désactivé pour debug

# Data Configuration
train_data_path: "./data/processed/text"
val_data_path: "./data/processed/text"  # Même path, split automatique
tokenizer_name: "microsoft/DialoGPT-large"

# Data Loading
sequence_bucketing: true
dynamic_padding: true
max_tokens_per_batch: 500000
bucket_boundaries: [1000, 5000, 10000, 25000, 50000, 100000]
dataloader_num_workers: 2
prefetch_factor: 2

# Checkpointing (Frequent for monitoring)
output_dir: "./checkpoints/stage1"
save_strategy: "steps"
save_steps: 1000
save_total_limit: 5
load_best_model_at_end: true

# Logging & Monitoring
logging_strategy: "steps" 
logging_steps: 50
eval_strategy: "steps"
eval_steps: 500
eval_delay: 1000

# Monitoring Tools
use_wandb: true
wandb_project: "ultra-ai-stage1"
wandb_run_name: "text-pretraining"
wandb_tags: ["stage1", "text-only", "mamba"]

use_tensorboard: true
tensorboard_log_dir: "./logs/stage1"

# Metrics to Track
monitor_metrics:
  - "loss/total"
  - "metrics/perplexity" 
  - "metrics/tokens_per_sec"
  - "hardware/gpu_memory"
  - "hardware/gpu_utilization"

# Early Stopping
early_stopping:
  enabled: true
  patience: 3
  min_delta: 0.01
  restore_best_weights: true

# Validation
evaluation:
  metrics: ["perplexity"]
  generation_samples: 10
  max_gen_length: 100
  temperature: 0.8

# Hardware Optimization (Local PC)
hardware:
  fp16_mixed_precision: true
  bf16_mixed_precision: false
  tf32_enabled: true
  cudnn_benchmark: true

# Memory Management (Aggressive for local)
cpu_offload_params: false  # Garder sur GPU si possible
cpu_offload_optimizer: false
pin_memory: true

# Distributed (Single GPU)
parallelism_strategy: "none"  # Single GPU
distributed: false

# Security
gradient_clipping: true
max_per_sample_grad_norm: 1.0

# Reproducibility (Debug)
reproducibility:
  enabled: true
  seed: 42
  deterministic: true

# Resource Monitoring
resource_monitoring:
  log_gpu_memory: true
  log_cpu_memory: true
  alert_thresholds:
    gpu_memory: 0.90
    cpu_memory: 0.85