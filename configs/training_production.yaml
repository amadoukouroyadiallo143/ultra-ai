# Ultra-AI Production Training Configuration
# Enterprise-grade setup for optimal performance and reliability

# Model Configuration - Production Scale
model_name: "ultra-ai-production"
model_variant: "ultra-52b"  # Start with 52B for production testing
d_model: 6144
vocab_size: 50432
max_seq_length: 50000000  # 50M tokens context (scaled from 100M for initial training)

# Architecture Distribution (Professional Optimized)
mamba_ratio: 0.70
attention_ratio: 0.20
moe_ratio: 0.08
multimodal_ratio: 0.02

# Mamba-2 Configuration (Optimized)
mamba_layers: 48
mamba_d_state: 128
mamba_d_conv: 4
mamba_expand: 2
mamba_dt_rank: "auto"

# Attention Configuration (High Performance)
attention_layers: 12
attention_heads: 48
attention_type: "linear"
linear_attention_threshold: 32768

# MoE Configuration (Load Balanced)
moe_layers: 6
num_experts: 128
moe_top_k: 2
expert_capacity_factor: 1.25
moe_balance_loss_weight: 0.01
load_balancing_strategy: "sinkhorn"

# Multimodal Configuration (Enterprise)
modalities: ["text", "image", "audio", "video"]
image_size: 384  # Higher resolution
audio_sample_rate: 16000
video_frames: 16  # More frames for better quality
max_image_tokens: 2048
max_audio_tokens: 3000
max_video_tokens: 1024

# Training Configuration - Enterprise Grade
batch_size: 2  # Dynamic based on sequence length
micro_batch_size: 1  # For gradient accumulation
gradient_accumulation_steps: 128  # Effective batch size = 256
max_gradient_accumulation_tokens: 2000000  # 2M tokens per update

learning_rate: 0.00008  # Conservative for stability
min_learning_rate: 0.000008
weight_decay: 0.01
max_grad_norm: 1.0
num_epochs: 5
warmup_ratio: 0.1

# Advanced Optimizer Configuration
optimizer: "adamw_8bit"  # Memory efficient
beta1: 0.9
beta2: 0.95
eps: 0.00000001
fused_optimizer: true
use_cpu_adam: false

# Learning Rate Schedule (Professional)
scheduler: "cosine_with_restarts"
cosine_restarts: 3
eta_min_ratio: 0.1
warmup_steps: 5000

# Training Efficiency & Stability
mixed_precision: "bf16"  # Best for modern GPUs
gradient_checkpointing: true
selective_checkpointing: true
use_flash_attention: true
compile_model: true
torch_compile_mode: "reduce-overhead"

# Memory Management (Professional)
cpu_offload_params: true
cpu_offload_optimizer: true
pin_memory: true
dataloader_num_workers: 8
prefetch_factor: 4

# Distributed Training (Multi-Node Ready)
parallelism_strategy: "deepspeed_stage3"
zero_stage: 3
offload_optimizer: true
offload_params: true
partition_activations: true
cpu_offload: true
overlap_comm: true
contiguous_gradients: true
bucket_cap_mb: 200

# Data Pipeline Configuration (High Quality)
train_data_path: "./data/curated_train"
val_data_path: "./data/curated_val"
test_data_path: "./data/curated_test"

# Data Quality Filters
min_text_length: 100
max_text_length: 1000000
text_quality_threshold: 0.8  # Perplexity-based filtering
duplicate_threshold: 0.85  # Near-duplicate detection
language_filter: ["en", "fr", "es", "de", "it"]  # Multi-language support

# Data Augmentation (Professional)
text_augmentation:
  enabled: true
  strategies: ["paraphrase", "back_translation", "noise_injection"]
  augmentation_prob: 0.1

multimodal_augmentation:
  image_transforms: ["resize", "crop", "color_jitter", "normalize"]
  audio_transforms: ["noise", "pitch_shift", "time_stretch"]
  video_transforms: ["temporal_crop", "spatial_crop", "normalize"]

# Tokenization (Advanced)
tokenizer_name: "microsoft/DialoGPT-large"
use_fast_tokenizer: true
sequence_bucketing: true
dynamic_padding: true
max_tokens_per_batch: 2000000
bucket_boundaries: [1000, 5000, 10000, 25000, 50000, 100000, 500000, 1000000]

# Checkpointing & Model Management (Enterprise)
output_dir: "./checkpoints/production"
checkpoint_dir: "./checkpoints/production/steps"
logging_dir: "./logs/production"

save_strategy: "steps"
save_steps: 2500
save_total_limit: 10
load_best_model_at_end: true
metric_for_best_model: "eval_loss"

# Advanced Checkpointing
smart_checkpointing: true
differential_checkpointing: true
checkpoint_compression: true
async_checkpointing: true

# Monitoring & Logging (Professional)
logging_strategy: "steps"
logging_steps: 50
eval_strategy: "steps"
eval_steps: 1000
eval_delay: 5000

# Advanced Monitoring
use_wandb: true
wandb_project: "ultra-ai-production"
wandb_entity: "ultra-ai-team"
wandb_run_name: null  # Auto-generated
wandb_tags: ["production", "ultra-ai", "multimodal"]

use_tensorboard: true
tensorboard_log_dir: "./logs/tensorboard"

monitor_metrics:
  - "loss/total"
  - "loss/moe_auxiliary"  
  - "loss/load_balancing"
  - "metrics/perplexity"
  - "metrics/throughput"
  - "hardware/gpu_memory"
  - "hardware/cpu_memory"
  - "hardware/gpu_utilization"

# Performance Profiling
profile_training: true
profiler_schedule:
  wait: 100
  warmup: 100
  active: 500
  repeat: 2

# Model Optimization & Quantization
optimization:
  enable_quantization: true
  quantization_method: "qat"  # Quantization Aware Training
  quantization_bits: 8
  quantize_moe: true
  quantize_attention: true
  quantize_mamba: true

# Knowledge Distillation (Advanced)
distillation:
  enabled: false  # Enable for smaller model variants
  teacher_model: null
  temperature: 4.0
  alpha: 0.7

# Continual Learning (Enterprise Feature)
continual_learning:
  enabled: true
  strategy: "ewc"  # Elastic Weight Consolidation
  ewc_lambda: 1000
  fisher_estimation_samples: 1000

# Security & Privacy (Professional)
security:
  gradient_clipping: true
  differential_privacy: false  # Enable if needed
  privacy_noise_multiplier: 1.0
  max_per_sample_grad_norm: 1.0

# Early Stopping (Robust)
early_stopping:
  enabled: true
  patience: 5
  min_delta: 0.001
  restore_best_weights: true

# Validation & Testing (Comprehensive)
evaluation:
  metrics: ["perplexity", "bleu", "rouge", "bertscore"]
  generation_samples: 100
  max_gen_length: 256
  temperature: 0.8
  top_p: 0.9
  num_beams: 4

# Hardware Optimization
hardware:
  fp16_mixed_precision: false  # Use bf16 instead
  bf16_mixed_precision: true
  tf32_enabled: true
  cudnn_benchmark: true
  cudnn_deterministic: false  # Set to true for reproducibility
  
# Reproducibility (Optional - impacts performance)
reproducibility:
  enabled: false
  seed: 42
  deterministic: false

# Resource Monitoring
resource_monitoring:
  log_gpu_memory: true
  log_cpu_memory: true  
  log_disk_usage: true
  alert_thresholds:
    gpu_memory: 0.95
    cpu_memory: 0.90
    disk_usage: 0.85

# Backup & Recovery
backup:
  enabled: true
  backup_interval: 10000  # steps
  backup_location: "./backups/production"
  max_backups: 5
  include_optimizer_state: false  # To save space

# Post-Training (Enterprise)
post_training:
  model_compression: true
  knowledge_distillation: false
  onnx_export: true
  quantization_calibration: true
  benchmark_suite: true