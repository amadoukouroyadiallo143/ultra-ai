# Deployment Guide

Comprehensive guide for deploying Ultra-AI models in production environments, from edge devices to enterprise-scale infrastructure.

## ðŸš€ Deployment Overview

Ultra-AI supports multiple deployment strategies:

- **ðŸŒ Server Deployment**: High-performance API servers
- **ðŸ“± Edge Deployment**: Mobile and embedded devices  
- **â˜ï¸ Cloud Deployment**: Scalable cloud infrastructure
- **ðŸ³ Container Deployment**: Docker and Kubernetes
- **âš¡ Optimized Inference**: ONNX, TensorRT, and custom kernels

## ðŸ“‹ Pre-Deployment Checklist

### System Requirements

| Deployment Type | CPU | RAM | GPU | Storage | Network |
|----------------|-----|-----|-----|---------|---------|
| **Edge** | ARM64/x64 | 4-16GB | Optional | 8-32GB | WiFi/4G |
| **Server** | 16+ cores | 64-256GB | A100/H100 | 1-10TB | 10Gb+ |
| **Cloud** | Variable | Variable | Variable | Variable | Variable |

### Model Preparation

```bash
# 1. Optimize model for deployment
python scripts/optimize.py \
    --model-path checkpoints/ultra-ai-3b \
    --optimization-level 2 \
    --target-platform server

# 2. Quantize for efficiency (optional)
python scripts/quantize.py \
    --model-path checkpoints/ultra-ai-3b \
    --quantization int8 \
    --calibration-data data/calibration.jsonl

# 3. Export to deployment format
python scripts/export.py \
    --model-path checkpoints/ultra-ai-3b \
    --format onnx \
    --output-path deploy/ultra-ai-3b.onnx
```

## ðŸŒ Server Deployment

### FastAPI Server Setup

```python
# deploy/server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import torch
import uvicorn
from ultra_ai_model import UltraAIModel, UltraAIConfig

app = FastAPI(
    title="Ultra-AI API Server",
    description="High-performance API server for Ultra-AI model inference",
    version="1.0.0"
)

# Request/Response models
class GenerationRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 100
    temperature: float = 0.8
    top_k: int = 50
    top_p: float = 0.9
    repetition_penalty: float = 1.1
    do_sample: bool = True

class GenerationResponse(BaseModel):
    generated_text: str
    input_length: int
    output_length: int
    generation_time: float
    tokens_per_second: float

class MultimodalRequest(BaseModel):
    text: str
    image_base64: Optional[str] = None
    audio_base64: Optional[str] = None
    max_new_tokens: int = 100

# Global model instance
model = None
tokenizer = None
device = None

@app.on_event("startup")
async def load_model():
    """Load model on startup"""
    global model, tokenizer, device
    
    print("Loading Ultra-AI model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Load configuration and model
    config = UltraAIConfig.load("config/ultra-3b.yaml")
    model = UltraAIModel.from_pretrained("checkpoints/ultra-ai-3b")
    model.to(device)
    model.eval()
    
    # Load tokenizer
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("checkpoints/ultra-ai-3b")
    
    print(f"Model loaded on {device}")

@app.post("/generate", response_model=GenerationResponse)
async def generate_text(request: GenerationRequest):
    """Generate text from prompt"""
    try:
        import time
        start_time = time.time()
        
        # Tokenize input
        input_ids = tokenizer.encode(
            request.prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=model.config.max_seq_length
        ).to(device)
        
        # Generate
        with torch.no_grad():
            output_ids = model.generate(
                input_ids,
                max_new_tokens=request.max_new_tokens,
                temperature=request.temperature,
                top_k=request.top_k,
                top_p=request.top_p,
                repetition_penalty=request.repetition_penalty,
                do_sample=request.do_sample,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # Decode response
        generated_text = tokenizer.decode(
            output_ids[0][input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        # Calculate metrics
        generation_time = time.time() - start_time
        input_length = input_ids.shape[1]
        output_length = output_ids.shape[1] - input_length
        tokens_per_second = output_length / generation_time
        
        return GenerationResponse(
            generated_text=generated_text,
            input_length=input_length,
            output_length=output_length,
            generation_time=generation_time,
            tokens_per_second=tokens_per_second
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/multimodal", response_model=GenerationResponse)
async def multimodal_generation(request: MultimodalRequest):
    """Multimodal generation endpoint"""
    try:
        import time
        import base64
        from PIL import Image
        import io
        
        start_time = time.time()
        
        # Process multimodal inputs
        multimodal_inputs = {}
        
        # Process image if provided
        if request.image_base64:
            image_data = base64.b64decode(request.image_base64)
            image = Image.open(io.BytesIO(image_data))
            # Convert image to tensor (implementation depends on your multimodal processor)
            multimodal_inputs['images'] = [image]
        
        # Process audio if provided  
        if request.audio_base64:
            audio_data = base64.b64decode(request.audio_base64)
            # Convert audio to tensor
            multimodal_inputs['audio'] = [audio_data]
        
        # Tokenize text
        input_ids = tokenizer.encode(
            request.text,
            return_tensors="pt"
        ).to(device)
        
        # Generate with multimodal inputs
        with torch.no_grad():
            output_ids = model.generate(
                input_ids,
                multimodal_inputs=multimodal_inputs if multimodal_inputs else None,
                max_new_tokens=request.max_new_tokens
            )
        
        # Decode and return
        generated_text = tokenizer.decode(
            output_ids[0][input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        
        return GenerationResponse(
            generated_text=generated_text,
            input_length=input_ids.shape[1],
            output_length=output_ids.shape[1] - input_ids.shape[1],
            generation_time=generation_time,
            tokens_per_second=(output_ids.shape[1] - input_ids.shape[1]) / generation_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": device,
        "cuda_available": torch.cuda.is_available(),
        "gpu_memory": torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
    }

@app.get("/model-info")
async def model_info():
    """Get model information"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    return {
        "model_name": "Ultra-AI",
        "version": "1.0.0",
        "parameters": sum(p.numel() for p in model.parameters()),
        "config": model.config.__dict__,
        "memory_usage": model.get_memory_usage()
    }

if __name__ == "__main__":
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8000,
        workers=1,  # Single worker for model loading
        reload=False
    )
```

### Running the Server

```bash
# Install server dependencies
pip install fastapi uvicorn python-multipart

# Run development server
python deploy/server.py

# Run production server with Gunicorn
pip install gunicorn
gunicorn deploy.server:app \
    --workers 1 \
    --worker-class uvicorn.workers.UvicornWorker \
    --bind 0.0.0.0:8000 \
    --timeout 300 \
    --max-requests 1000
```

### Server Configuration

```yaml
# deploy/server_config.yaml
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  max_requests: 1000
  timeout: 300
  
model:
  path: "checkpoints/ultra-ai-3b"
  device: "auto"  # auto, cuda, cpu
  max_seq_length: 100000
  batch_size: 1
  
optimization:
  torch_compile: true
  mixed_precision: true
  gradient_checkpointing: false
  
caching:
  enable_kv_cache: true
  max_cache_size: 1000
  
monitoring:
  enable_metrics: true
  log_level: "INFO"
  prometheus_port: 9090
```

## ðŸ³ Container Deployment

### Dockerfile

```dockerfile
# deploy/Dockerfile
FROM nvidia/cuda:12.1-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create app directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Install Ultra-AI
COPY . .
RUN pip3 install -e .

# Create non-root user
RUN useradd -m -u 1000 ultraai
RUN chown -R ultraai:ultraai /app
USER ultraai

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run server
CMD ["python3", "deploy/server.py"]
```

### Docker Compose

```yaml
# deploy/docker-compose.yml
version: '3.8'

services:
  ultra-ai-server:
    build:
      context: .
      dockerfile: deploy/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./checkpoints:/app/checkpoints
      - ./logs:/app/logs
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - OMP_NUM_THREADS=4
      - TOKENIZERS_PARALLELISM=false
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 32G
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./deploy/nginx.conf:/etc/nginx/nginx.conf
      - ./certs:/etc/nginx/certs
    depends_on:
      - ultra-ai-server
    restart: unless-stopped
    
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./deploy/prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped
    
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped

volumes:
  grafana-data:
```

### Building and Running

```bash
# Build image
docker build -t ultra-ai:latest -f deploy/Dockerfile .

# Run single container
docker run --gpus all -p 8000:8000 \
    -v ./checkpoints:/app/checkpoints \
    ultra-ai:latest

# Run with Docker Compose
docker-compose -f deploy/docker-compose.yml up -d

# Scale services
docker-compose -f deploy/docker-compose.yml up --scale ultra-ai-server=3 -d
```

## â˜ï¸ Cloud Deployment

### AWS Deployment

#### ECS with Fargate

```json
{
  "family": "ultra-ai-task",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "4096",
  "memory": "16384",
  "executionRoleArn": "arn:aws:iam::account:role/ecsTaskExecutionRole",
  "taskRoleArn": "arn:aws:iam::account:role/ecsTaskRole",
  "containerDefinitions": [
    {
      "name": "ultra-ai-container",
      "image": "your-account.dkr.ecr.region.amazonaws.com/ultra-ai:latest",
      "portMappings": [
        {
          "containerPort": 8000,
          "protocol": "tcp"
        }
      ],
      "essential": true,
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/ultra-ai",
          "awslogs-region": "us-west-2",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "environment": [
        {
          "name": "MODEL_PATH",
          "value": "/app/checkpoints/ultra-ai-3b"
        }
      ],
      "mountPoints": [
        {
          "sourceVolume": "model-storage",
          "containerPath": "/app/checkpoints",
          "readOnly": true
        }
      ]
    }
  ],
  "volumes": [
    {
      "name": "model-storage",
      "efsVolumeConfiguration": {
        "fileSystemId": "fs-12345678",
        "rootDirectory": "/models"
      }
    }
  ]
}
```

#### EC2 with GPU

```bash
# Launch EC2 instance with Deep Learning AMI
aws ec2 run-instances \
    --image-id ami-0c94855b9da08d0b1 \
    --instance-type g5.2xlarge \
    --key-name your-key \
    --security-group-ids sg-12345678 \
    --subnet-id subnet-12345678 \
    --user-data file://deploy/user-data.sh

# User data script (deploy/user-data.sh)
#!/bin/bash
cd /home/ubuntu
git clone https://github.com/your-org/ultra-ai-model.git
cd ultra-ai-model
pip install -e .
python deploy/server.py
```

### Google Cloud Platform

```yaml
# deploy/gcp-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ultra-ai-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ultra-ai
  template:
    metadata:
      labels:
        app: ultra-ai
    spec:
      containers:
      - name: ultra-ai-container
        image: gcr.io/your-project/ultra-ai:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "4"
          limits:
            nvidia.com/gpu: 1
            memory: "32Gi"
            cpu: "8"
        env:
        - name: MODEL_PATH
          value: "/app/checkpoints/ultra-ai-3b"
        volumeMounts:
        - name: model-storage
          mountPath: /app/checkpoints
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc
      nodeSelector:
        accelerator: nvidia-tesla-t4
---
apiVersion: v1
kind: Service
metadata:
  name: ultra-ai-service
spec:
  selector:
    app: ultra-ai
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### Azure Deployment

```yaml
# deploy/azure-container-instance.yaml
apiVersion: 2019-12-01
location: eastus
name: ultra-ai-container-group
properties:
  containers:
  - name: ultra-ai-container
    properties:
      image: your-registry.azurecr.io/ultra-ai:latest
      resources:
        requests:
          cpu: 4
          memoryInGb: 16
          gpu:
            count: 1
            sku: V100
      ports:
      - port: 8000
        protocol: TCP
      environmentVariables:
      - name: MODEL_PATH
        value: /app/checkpoints/ultra-ai-3b
      volumeMounts:
      - name: model-storage
        mountPath: /app/checkpoints
  osType: Linux
  restartPolicy: Always
  ipAddress:
    type: Public
    ports:
    - protocol: tcp
      port: 8000
  volumes:
  - name: model-storage
    azureFile:
      shareName: models
      storageAccountName: your-storage-account
      storageAccountKey: your-storage-key
tags:
  environment: production
  application: ultra-ai
```

## ðŸ“± Edge Deployment

### Mobile Deployment (iOS/Android)

```python
# deploy/mobile_optimizer.py
import torch
import torch.utils.mobile_optimizer as mobile_optimizer
from ultra_ai_model import UltraAIModel, UltraAIConfig

def optimize_for_mobile(model_path: str, output_path: str):
    """
    Optimize Ultra-AI model for mobile deployment
    """
    # Load model
    config = UltraAIConfig.load("config/ultra-edge.yaml")
    model = UltraAIModel.from_pretrained(model_path)
    model.eval()
    
    # Quantize to INT8
    model_int8 = torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear, torch.nn.Conv1d},
        dtype=torch.qint8
    )
    
    # Optimize for mobile
    scripted_model = torch.jit.script(model_int8)
    optimized_model = mobile_optimizer.optimize_for_mobile(scripted_model)
    
    # Save mobile-optimized model
    optimized_model._save_for_lite_interpreter(output_path)
    
    print(f"Mobile-optimized model saved to {output_path}")
    
    # Estimate model size
    import os
    size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"Model size: {size_mb:.2f} MB")

# Usage
optimize_for_mobile(
    "checkpoints/ultra-ai-edge",
    "deploy/ultra-ai-mobile.ptl"
)
```

### Embedded Systems (ARM/Raspberry Pi)

```python
# deploy/embedded_deploy.py
import torch
import torchvision
from ultra_ai_model import UltraAIModel, UltraAIConfig

class EmbeddedUltraAI:
    """
    Optimized Ultra-AI for embedded systems
    """
    
    def __init__(self, model_path: str, device: str = "cpu"):
        self.device = device
        
        # Load lightweight config
        config = UltraAIConfig.load("config/ultra-edge.yaml")
        
        # Load and optimize model
        self.model = UltraAIModel.from_pretrained(model_path)
        self.model.to(device)
        self.model.eval()
        
        # Apply optimizations
        self._optimize_for_embedded()
    
    def _optimize_for_embedded(self):
        """Apply embedded-specific optimizations"""
        
        # Enable CPU optimizations
        torch.set_num_threads(4)
        
        # Quantize model
        self.model = torch.quantization.quantize_dynamic(
            self.model,
            {torch.nn.Linear},
            dtype=torch.qint8
        )
        
        # Compile with TorchScript
        self.model = torch.jit.script(self.model)
        self.model = torch.jit.optimize_for_inference(self.model)
    
    def generate(self, prompt: str, max_length: int = 50):
        """Generate text with memory constraints"""
        
        # Simple tokenization (replace with actual tokenizer)
        tokens = prompt.split()
        input_ids = torch.tensor([hash(token) % 10000 for token in tokens]).unsqueeze(0)
        
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_new_tokens=max_length,
                do_sample=False,  # Greedy for speed
                temperature=1.0
            )
        
        return outputs
```

### WebAssembly (WASM) Deployment

```python
# deploy/wasm_export.py
import torch
from ultra_ai_model import UltraAIModel, UltraAIConfig

def export_to_wasm(model_path: str, output_path: str):
    """
    Export Ultra-AI model to WebAssembly format
    """
    # Load minimal model
    config = UltraAIConfig.load("config/ultra-edge.yaml")
    model = UltraAIModel.from_pretrained(model_path)
    model.eval()
    
    # Quantize and optimize
    model_int8 = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )
    
    # Create example input
    example_input = torch.randint(0, 1000, (1, 10))
    
    # Export to ONNX first
    torch.onnx.export(
        model_int8,
        example_input,
        f"{output_path}.onnx",
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch_size", 1: "sequence_length"},
            "logits": {0: "batch_size", 1: "sequence_length"}
        }
    )
    
    print(f"ONNX model exported to {output_path}.onnx")
    print("Use onnx2wasm tool to convert to WebAssembly")
```

## âš¡ Performance Optimization

### ONNX Runtime Optimization

```python
# deploy/onnx_deploy.py
import onnxruntime as ort
import numpy as np
from ultra_ai_model import UltraAIModel, UltraAIConfig

class ONNXUltraAI:
    """
    ONNX Runtime deployment for Ultra-AI
    """
    
    def __init__(self, onnx_path: str, providers: list = None):
        if providers is None:
            providers = [
                "CUDAExecutionProvider",
                "CPUExecutionProvider"
            ]
        
        # Configure ONNX Runtime session
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.intra_op_num_threads = 4
        sess_options.inter_op_num_threads = 4
        
        # Create inference session
        self.session = ort.InferenceSession(
            onnx_path,
            sess_options=sess_options,
            providers=providers
        )
        
        # Get input/output names
        self.input_names = [input.name for input in self.session.get_inputs()]
        self.output_names = [output.name for output in self.session.get_outputs()]
    
    def generate(self, input_ids: np.ndarray) -> np.ndarray:
        """Run inference with ONNX Runtime"""
        
        inputs = {self.input_names[0]: input_ids}
        outputs = self.session.run(self.output_names, inputs)
        
        return outputs[0]  # Return logits
    
    def benchmark(self, num_runs: int = 100):
        """Benchmark inference performance"""
        import time
        
        # Create dummy input
        input_ids = np.random.randint(0, 1000, (1, 100), dtype=np.int64)
        
        # Warmup
        for _ in range(10):
            self.generate(input_ids)
        
        # Benchmark
        start_time = time.time()
        for _ in range(num_runs):
            self.generate(input_ids)
        
        avg_time = (time.time() - start_time) / num_runs
        throughput = input_ids.shape[1] / avg_time
        
        print(f"Average inference time: {avg_time*1000:.2f}ms")
        print(f"Throughput: {throughput:.2f} tokens/sec")

# Export to ONNX
def export_to_onnx(model_path: str, output_path: str):
    """Export Ultra-AI model to ONNX format"""
    
    config = UltraAIConfig.load("config/ultra-3b.yaml")
    model = UltraAIModel.from_pretrained(model_path)
    model.eval()
    
    # Create example input
    batch_size, seq_length = 1, 100
    example_input = torch.randint(0, config.vocab_size, (batch_size, seq_length))
    
    # Export to ONNX
    torch.onnx.export(
        model,
        example_input,
        output_path,
        input_names=["input_ids"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch_size", 1: "sequence_length"},
            "logits": {0: "batch_size", 1: "sequence_length"}
        },
        opset_version=17
    )
    
    print(f"ONNX model exported to {output_path}")
```

### TensorRT Optimization

```python
# deploy/tensorrt_deploy.py
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np

class TensorRTUltraAI:
    """
    TensorRT deployment for Ultra-AI
    """
    
    def __init__(self, engine_path: str):
        # Load TensorRT engine
        with open(engine_path, 'rb') as f:
            self.runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
            self.engine = self.runtime.deserialize_cuda_engine(f.read())
        
        self.context = self.engine.create_execution_context()
        
        # Allocate GPU memory
        self.inputs = []
        self.outputs = []
        self.bindings = []
        
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding)) * \
                   self.engine.max_batch_size
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))
            
            # Allocate host and device buffers
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            
            self.bindings.append(int(device_mem))
            
            if self.engine.binding_is_input(binding):
                self.inputs.append({'host': host_mem, 'device': device_mem})
            else:
                self.outputs.append({'host': host_mem, 'device': device_mem})
    
    def infer(self, input_data: np.ndarray) -> np.ndarray:
        """Run inference with TensorRT"""
        
        # Copy input data to GPU
        np.copyto(self.inputs[0]['host'], input_data.ravel())
        cuda.memcpy_htod(self.inputs[0]['device'], self.inputs[0]['host'])
        
        # Run inference
        self.context.execute_v2(bindings=self.bindings)
        
        # Copy output from GPU
        cuda.memcpy_dtoh(self.outputs[0]['host'], self.outputs[0]['device'])
        
        return self.outputs[0]['host'].reshape(-1, input_data.shape[1], -1)

def build_tensorrt_engine(onnx_path: str, engine_path: str):
    """Build TensorRT engine from ONNX model"""
    
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, logger)
    
    # Parse ONNX model
    with open(onnx_path, 'rb') as model:
        if not parser.parse(model.read()):
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            return None
    
    # Build engine
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB
    config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16 precision
    
    serialized_engine = builder.build_serialized_network(network, config)
    
    # Save engine
    with open(engine_path, 'wb') as f:
        f.write(serialized_engine)
    
    print(f"TensorRT engine saved to {engine_path}")
```

## ðŸ”§ Load Balancing & Scaling

### NGINX Configuration

```nginx
# deploy/nginx.conf
upstream ultra_ai_backend {
    least_conn;
    server ultra-ai-server-1:8000 max_fails=3 fail_timeout=30s;
    server ultra-ai-server-2:8000 max_fails=3 fail_timeout=30s;
    server ultra-ai-server-3:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name api.ultra-ai.com;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;
    
    # Timeouts
    proxy_connect_timeout 60s;
    proxy_send_timeout 300s;
    proxy_read_timeout 300s;
    
    # Buffer sizes
    proxy_buffer_size 4k;
    proxy_buffers 8 4k;
    proxy_busy_buffers_size 8k;
    
    location /generate {
        proxy_pass http://ultra_ai_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Enable request buffering
        proxy_request_buffering on;
        client_max_body_size 10M;
    }
    
    location /health {
        proxy_pass http://ultra_ai_backend;
        access_log off;
    }
    
    # Static files and caching
    location /static/ {
        alias /var/www/static/;
        expires 30d;
        add_header Cache-Control "public, immutable";
    }
}
```

### Kubernetes Auto-scaling

```yaml
# deploy/k8s-autoscaling.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ultra-ai-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ultra-ai-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
---
apiVersion: v1
kind: Service
metadata:
  name: ultra-ai-service
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
spec:
  selector:
    app: ultra-ai
  ports:
  - name: http
    port: 80
    targetPort: 8000
  - name: metrics
    port: 9090
    targetPort: 9090
  type: ClusterIP
```

## ðŸ“Š Monitoring & Observability

### Prometheus Metrics

```python
# deploy/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import functools

# Define metrics
REQUEST_COUNT = Counter(
    'ultra_ai_requests_total',
    'Total number of requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'ultra_ai_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

MODEL_MEMORY_USAGE = Gauge(
    'ultra_ai_memory_usage_bytes',
    'Model memory usage in bytes'
)

TOKENS_GENERATED = Counter(
    'ultra_ai_tokens_generated_total',
    'Total tokens generated'
)

ACTIVE_REQUESTS = Gauge(
    'ultra_ai_active_requests',
    'Number of active requests'
)

def track_metrics(func):
    """Decorator to track metrics for API endpoints"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        ACTIVE_REQUESTS.inc()
        
        try:
            result = func(*args, **kwargs)
            REQUEST_COUNT.labels(
                method="POST",
                endpoint=func.__name__,
                status="success"
            ).inc()
            return result
        except Exception as e:
            REQUEST_COUNT.labels(
                method="POST",
                endpoint=func.__name__,
                status="error"
            ).inc()
            raise
        finally:
            duration = time.time() - start_time
            REQUEST_DURATION.labels(
                method="POST",
                endpoint=func.__name__
            ).observe(duration)
            ACTIVE_REQUESTS.dec()
    
    return wrapper

# Start metrics server
start_http_server(9090)
```

### Grafana Dashboard

```json
{
  "dashboard": {
    "title": "Ultra-AI Model Performance",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ultra_ai_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(ultra_ai_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(ultra_ai_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "type": "graph", 
        "targets": [
          {
            "expr": "ultra_ai_memory_usage_bytes / 1024 / 1024 / 1024",
            "legendFormat": "Memory Usage (GB)"
          }
        ]
      },
      {
        "title": "Token Generation Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ultra_ai_tokens_generated_total[5m])",
            "legendFormat": "Tokens/sec"
          }
        ]
      }
    ]
  }
}
```

## ðŸ”’ Security & Best Practices

### Security Configuration

```python
# deploy/security.py
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
import jwt
import os

app = FastAPI()

# Security middleware
app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["api.ultra-ai.com", "*.ultra-ai.com"]
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://app.ultra-ai.com"],
    allow_methods=["POST", "GET"],
    allow_headers=["Authorization", "Content-Type"],
    allow_credentials=False
)

security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verify JWT token"""
    try:
        payload = jwt.decode(
            credentials.credentials,
            os.getenv("JWT_SECRET_KEY"),
            algorithms=["HS256"]
        )
        return payload
    except jwt.InvalidTokenError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication token"
        )

@app.middleware("http")
async def add_security_headers(request, call_next):
    """Add security headers to all responses"""
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    return response
```

### Rate Limiting

```python
# deploy/rate_limiting.py
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from slowapi.middleware import SlowAPIMiddleware

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
app.add_middleware(SlowAPIMiddleware)

@app.post("/generate")
@limiter.limit("10/minute")
async def generate_text(request: Request, generation_request: GenerationRequest):
    """Rate-limited text generation endpoint"""
    # Implementation here
    pass
```

This deployment guide provides comprehensive coverage of deploying Ultra-AI models across various environments and scales, from edge devices to enterprise cloud infrastructure, with proper monitoring, security, and optimization strategies.