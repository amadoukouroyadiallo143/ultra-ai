# Ultra-AI Base Configuration
# Revolutionary hybrid architecture: Mamba-2 + Attention + MoE + Multimodal

model_name: "ultra-ai-390b"
d_model: 2560
vocab_size: 50432
max_seq_length: 100000000  # 100M tokens context

# Hybrid architecture ratios (must sum to 1.0)
mamba_ratio: 0.70    # 70% Mamba-2 backbone
attention_ratio: 0.20 # 20% Advanced attention 
moe_ratio: 0.08      # 8% Mixture of Experts
multimodal_ratio: 0.02 # 2% Multimodal fusion

# Mamba-2 Configuration (70% of architecture)
mamba_layers: 56
mamba_d_state: 128
mamba_d_conv: 4
mamba_expand: 2
mamba_dt_rank: "auto"

# Attention Configuration (20% - integrated 1:7 ratio with Mamba)
attention_layers: 8
attention_heads: 16
attention_type: "linear"  # linear, cca, in_attention

# MoE Configuration (8% of architecture) 
moe_layers: 4
num_experts: 256
moe_top_k: 2
expert_capacity_factor: 1.25
moe_balance_loss_weight: 0.01

# Multimodal Configuration (2% of architecture)
modalities: ["text", "image", "audio", "video"]
image_size: 224
audio_sample_rate: 16000
video_frames: 8
max_image_tokens: 1024
max_audio_tokens: 1500
max_video_tokens: 512

# Training Configuration for Ultra-Long Context
batch_size: 1  # Ultra-long sequences require small batches
gradient_accumulation_steps: 64  # Effective batch size = 64
learning_rate: 0.0001
min_learning_rate: 0.00001
weight_decay: 0.01
max_grad_norm: 1.0
num_epochs: 3

# Optimizer Configuration
optimizer: "adamw"  # adamw, lion, sophia, galore
beta1: 0.9
beta2: 0.95
eps: 0.00000001

# Scheduler Configuration
scheduler: "cosine"
warmup_steps: 2000

# Training Efficiency
mixed_precision: true
gradient_checkpointing: true
use_flash_attention: true
compile_model: true

# Distributed Training 
parallelism_strategy: "deepspeed"  # data_parallel, model_parallel, pipeline_parallel, deepspeed, fsdp
zero_stage: 2
cpu_offload: true

# Data Configuration
train_data_path: "./data/train"
val_data_path: "./data/val" 
tokenizer_name: "microsoft/DialoGPT-large"
sequence_bucketing: true
max_tokens_per_batch: 1000000  # 1M tokens per batch

# Checkpointing and Logging
output_dir: "./checkpoints"
logging_steps: 100
eval_steps: 1000
save_steps: 5000
max_checkpoints: 5

# Monitoring
use_wandb: true
wandb_project: "ultra-ai-model"
monitor_memory: true
profile_training: false

# Model Optimization
use_quantization: false
quantization_method: "qat"  # qat, ptq, qlora
quantization_bits: 8

# Advanced Features
continual_learning: true
catastrophic_forgetting_prevention: "ewc"  # ewc, l2, replay

# Security
gradient_clipping: true
differential_privacy: false
privacy_noise_multiplier: 1.0

# Edge Deployment
edge_optimization: false
target_device: "gpu"  # cpu, gpu, mobile, edge
model_compression: "none"  # none, pruning, distillation, both